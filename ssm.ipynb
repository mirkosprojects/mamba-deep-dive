{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5419590b",
   "metadata": {},
   "source": [
    "# Demonstration of SSM Functionality\n",
    "This Jupyter Notebook gives an overview of the functionality of State Space Models (SSMs).\n",
    "\n",
    "SSMs evolve from Control Theory and are defined by the state and output equations:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x'(t) &= Ax(t) + Bu(t) &&\\text{(State Equation)}\\\\\n",
    "y(t) &= Cx(t) + Du(t) &&\\text{(Output Equation)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "With:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "u(t)&: \\text{Input Signal}\\\\\n",
    "x(t)&: \\text{Latent State}\\\\\n",
    "y(t)&: \\text{Output Signal}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e9f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from celluloid import Camera\n",
    "from IPython.display import Image, Video, HTML\n",
    "from utils import animate_spring_system\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcfe2a1",
   "metadata": {},
   "source": [
    "## 1) Discretization\n",
    "To use the continuous SSM in a discrete context, we discretize the system. The different discretization techniques are explained in detail in the research article so we won't go into detail. We use the `Bilinear Transformation`, which gives us a discretized system:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_k &= \\bar{A}x_{k-1} + \\bar{B}u_{k} &&\\text{(State Equation)}\\\\\n",
    "y_k &= Cx_k &&\\text{(Output Equation)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that we replaced the time-dependent variables input $u(t)$, output $y(t)$ and state $x(t)$ in the equations with discrete variables $u_k$, $y_k$ and $x_k$.\n",
    "\n",
    "The discretized matrices $\\bar{A}$, $\\bar{B}$ and $\\bar{C}$ can be calculated from the continuous matrices $A$, $B$, $C$ as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\bar{A} &= (I-\\frac{\\Delta}{2}A)^{-1}\\cdot(I+\\frac{\\Delta}{2}A)\\\\\n",
    "\\bar{B} &= (I-\\frac{\\Delta}{2}A)^{-1}\\cdot \\Delta B\\\\\n",
    "\\bar{C} &= C\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63179f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(A: np.ndarray, B: np.ndarray, dt: float) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Converts continuous-time matrices A, B into discrete-time versions using the bilinear transform.\n",
    "\n",
    "    Parameters:\n",
    "        A (ndarray): continuous-time state transition matrix\n",
    "        B (ndarray): continuous-time input matrix\n",
    "        dt (float): time step for discretization\n",
    "    \n",
    "    Returns:\n",
    "        (A_d, B_d) (ndarray, ndarray): discrete-time state transition matrix A_d and discrete-time input matrix B_d\n",
    "    \"\"\"\n",
    "    I = np.eye(A.shape[0])\n",
    "    A_d = np.linalg.inv(I - 0.5 * A * dt) @ (I + 0.5 * A * dt)\n",
    "    B_d = np.linalg.inv(I - 0.5 * A * dt) @ (B * dt)\n",
    "\n",
    "    # Alternatively, using solve_triangular for better numerical stability and faster computation\n",
    "    # A_d = la.solve_triangular(I - 0.5 * A * dt, I + 0.5 * A * dt, lower=True)\n",
    "    # B_d = la.solve_triangular(I - 0.5 * A * dt, B * dt, lower=True)\n",
    "    return A_d, B_d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761433da",
   "metadata": {},
   "source": [
    "## 2) Recurrent Calculation of the Output\n",
    "Using the discretized system, we can recurrently calculate the state $x_k$ and output $y_k$, which is implemented in the `run_SSM()` function. Note that the calculations inside the for-loop are identical to the discretized system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637d9bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_SSM(A: np.ndarray, B: np.ndarray, C: np.ndarray, x0: np.ndarray, u: np.ndarray):\n",
    "    \"\"\"Runs the SSM recurrently over all inputs u\"\"\"\n",
    "\n",
    "    # Convenience Variables\n",
    "    seq_len = u.shape[0]\n",
    "    state_size = x0.shape[0]\n",
    "\n",
    "    # Ensure 1D arrays for x0, u, B, C\n",
    "    x0 = x0.ravel()\n",
    "    u = u.ravel()\n",
    "    B = B.ravel()\n",
    "    C = C.ravel()\n",
    "\n",
    "    # Initialize input, state and output arrays\n",
    "    y = np.zeros(seq_len + 1)\n",
    "    x = np.zeros((seq_len + 1, state_size))\n",
    "    x[0] = x0\n",
    "    u = np.concatenate([[0], u])    # Prepend 0 to u, so the indexing matches with x and y\n",
    "    \n",
    "    # Iterate over the input\n",
    "    for k in range(1, seq_len + 1):\n",
    "        x[k] = A @ x[k-1] + B * u[k]\n",
    "        y[k] = C @ x[k]\n",
    "    return y[1:]    # Remove the prepended 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b133d9ab",
   "metadata": {},
   "source": [
    "## 3) Example: Spring System\n",
    "The following code gives an example system of a mass attached to a wall with a spring.\n",
    "We give it an input force $u$ and using the SSM, we can calculate the position of the mass $y$.\n",
    "\n",
    "The derivation for the $A$, $B$ and $C$ matrices is given in the article, they are defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A &= \\begin{bmatrix}\n",
    "0 & 1\\\\\n",
    "-\\frac{k}{m} & -\\frac{b}{m}\n",
    "\\end{bmatrix}\\\\\n",
    "B &= \\begin{bmatrix}\n",
    "0\\\\\n",
    "\\frac{1}{m}\n",
    "\\end{bmatrix}\\\\\n",
    "C &= \\begin{bmatrix}\n",
    "1 & 0\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "k&: \\text{spring constant}\\\\\n",
    "b&: \\text{friction coefficient}\\\\\n",
    "m&: \\text{mass}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83eb860",
   "metadata": {},
   "source": [
    "### 3.1) Running the SSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db9bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://srush.github.io/annotated-s4/\n",
    "\n",
    "# Parameters\n",
    "k = 40\n",
    "b = 5\n",
    "m = 1\n",
    "\n",
    "# Initialize SSM Matrices\n",
    "A = np.array([[0, 1], \n",
    "              [-k/m, -b/m]])\n",
    "B = np.array([[0], \n",
    "              [1.0/m]])\n",
    "C = np.array([[1.0, 0]])\n",
    "\n",
    "# Generate an Input Signal\n",
    "L = 200\n",
    "step = 1.0 / L\n",
    "ks = np.arange(L)\n",
    "x = np.sin(10 * ks * step)\n",
    "u = x * (x > 0.5)\n",
    "\n",
    "# Discretize the SSM\n",
    "A, B = discretize(A, B, step)\n",
    "\n",
    "# Set the initial state\n",
    "# x_0 = np.ones_like(B) * 0.001\n",
    "x_0 = np.zeros_like(B)\n",
    "\n",
    "# Run the SSM\n",
    "y = run_SSM(A, B, C, x_0, u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfa2829",
   "metadata": {},
   "source": [
    "### 3.2) Animating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71a35c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference Code for animation: https://srush.github.io/annotated-s4/\n",
    "anim = animate_spring_system(u, y)\n",
    "\n",
    "# Save the animation\n",
    "# anim.save(\"media/ssm.gif\", dpi=150)\n",
    "# Image(\"line.gif\")\n",
    "# anim.save(\"line.mp4\", fps=25)\n",
    "# Video(\"line.mp4\")\n",
    "\n",
    "# Show the animation\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3152b5",
   "metadata": {},
   "source": [
    "## 4) Convolution\n",
    "\n",
    "We can speed up the calculation of output $y$ by replacing the recurrent calculation of $y$ with a convolution, which makes it parallelizable.\n",
    "\n",
    "$$\n",
    "y = \\bar{K} * u\n",
    "$$\n",
    "\n",
    "The derivation for the convolution kernel $K$ can be seen in the article, it looks as follows:\n",
    "\n",
    "$$\n",
    "\\bar{K} = (\\bar{C} \\bar{B}, \\bar{C} \\bar{A}^1 \\bar{B}, \\dots, \\bar{C} \\bar{A}^k \\bar{B})\n",
    "$$\n",
    "\n",
    "This can be easily implemented using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b76996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_K(A: np.ndarray, B: np.ndarray, C: np.ndarray, L: int) -> np.ndarray:\n",
    "    \"\"\"Calculate the convolution kernel K\"\"\"\n",
    "    K = np.array([C @ np.linalg.matrix_power(A, i) @ B for i in range(L)])\n",
    "    return K.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d1282d",
   "metadata": {},
   "source": [
    "### 4.1) Plotting the results\n",
    "To compare recurrent and convolutional calculation, we plot both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea34a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the convolution kernel K\n",
    "K = calculate_K(A, B, C, L)\n",
    "\n",
    "# Calculate y using the convolution K * u\n",
    "y_conv = np.convolve(K, u)[:L]\n",
    "\n",
    "# Plot the results\n",
    "fig, axs = plt.subplots(3, figsize=(12, 6))\n",
    "axs[0].set_title(\"Force $u_k$\")\n",
    "axs[1].set_title(\"Recurrent Calculation of Position $y_k$\")\n",
    "axs[2].set_title(\"Convolutional Calculation of Position $y_k$\")\n",
    "axs[0].plot(u, color=\"red\")\n",
    "axs[1].plot(y, color=\"blue\")\n",
    "axs[2].plot(y_conv, color=\"blue\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d184fb9",
   "metadata": {},
   "source": [
    "## 5) Linear State Space Layer (LSSL)\n",
    "**TODO:** After presenting all concepts of the SSM (Discretization, Recurrent Representation, Convolution), add some code that puts everything into one LSSL class.\n",
    "\n",
    "Reference: https://tinkerd.net/blog/machine-learning/state-space-models/#linear-state-space-layers-lssl\n",
    "\n",
    "### 5.1) Adding Channels\n",
    "\n",
    "Instead of only having one input and output channel in $u$ and $y$, we can increase their dimensionality while keeping the dimensionality of the state $x$ identical. The new variables look as follows:\n",
    "\n",
    "| Symbol | Previous Shape | New Shape    |\n",
    "| ------ | -------------- | ------------ |\n",
    "| $u$    | $L$            | $L \\times H$ |\n",
    "| $x$    | $N$            | $N$          |\n",
    "| $y$    | $L$            | $L \\times M$ |\n",
    "\n",
    "Where:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "N &: \\text{State Size}\\\\\n",
    "H &: \\text{Input Channels}\\\\\n",
    "M &: \\text{Output Channels}\\\\\n",
    "L &: \\text{Sequence Length}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We therefore have to increase the dimensionality of $B$, $C$ and $D$ aswell:\n",
    "\n",
    "| Matrix | Previous Shape | New Shape             |\n",
    "| ------ | -------------- | --------------------- |\n",
    "| $A$    | $N \\times N$   | $N \\times N$          |\n",
    "| $B$    | $N$            | $H \\times N$          |\n",
    "| $C$    | $N \\times M$   | $H \\times N \\times M$ |\n",
    "| $D$    | $H$            | $H \\times M$          |\n",
    "\n",
    "### 5.2) Linear State Space Layer (LSSL)\n",
    "We can put the concepts from above together to build an SSM by stacking multiple LSSLs. This is the equivalent of stacking convolutional layers to form a Convolutional Neural Network (CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2be456",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSSL(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, state_size, initial_dt=0.01):\n",
    "        super(LSSL, self).__init__()\n",
    "\n",
    "        self.input_size = input_size    # H\n",
    "        self.output_size = output_size  # M\n",
    "        self.state_size = state_size    # N\n",
    "\n",
    "        # Continuous-time parameters\n",
    "        self.A = torch.nn.Parameter(torch.randn(self.state_size, self.state_size))\n",
    "        self.B = torch.nn.Parameter(torch.randn(self.input_size, self.state_size))\n",
    "        self.dt = torch.nn.Parameter(torch.ones(self.input_size) * initial_dt)\n",
    "\n",
    "        self.C = torch.nn.Parameter(torch.randn(self.input_size, self.output_size, self.state_size))\n",
    "        self.D = torch.nn.Parameter(torch.randn(self.input_size, self.output_size))\n",
    "        self.output_proj = torch.nn.Parameter(torch.randn(self.input_size * self.output_size, self.input_size))\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        # Discretize the system\n",
    "        A_discrete, B_discrete = discretize(self.A, self.B, self.dt)\n",
    "\n",
    "        batch_size, sequence_length = input.shape[0], input.shape[1]\n",
    "\n",
    "        # Initialize state to 0\n",
    "        x = torch.zeros(batch_size, self.input_size, self.state_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = []\n",
    "        for i in range(sequence_length):\n",
    "\n",
    "            current_input = input[:, i, :]\n",
    "            \n",
    "            # Update the current state\n",
    "            Ax = (A_discrete @ x.unsqueeze(-1)).reshape(*x.shape)\n",
    "            Bu = torch.einsum('hn, bh -> bhn', B_discrete, current_input)\n",
    "            x = Ax + Bu\n",
    "\n",
    "            # Compute the output at the current time step\n",
    "            Cx = torch.einsum('hmn, bhn -> bhm', self.C, x)\n",
    "            Du = torch.einsum('hm, bh -> bhm', self.D, current_input)\n",
    "            y = Cx + Du\n",
    "            \n",
    "            outputs.append(y)\n",
    "        \n",
    "        y = torch.stack(outputs, dim=1)\n",
    "\n",
    "        # Output projection\n",
    "        output = y.view(batch_size, sequence_length, -1) @ self.output_proj\n",
    "        return output\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
