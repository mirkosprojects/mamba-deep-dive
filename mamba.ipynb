{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7b0d43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: https://github.com/johnma2006/mamba-minimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3be24452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/deepLearning/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from einops import rearrange, repeat\n",
    "from transformers import AutoTokenizer\n",
    "from utils.mamba import generate, from_pretrained, ssm, RMSNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518b4254",
   "metadata": {},
   "source": [
    "# Model Definition\n",
    "We build out MAMBA from the innermost `MAMBA Block`, to the surrounding `Resiudal Block`, to the full `MAMBA Model`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64ffdd8",
   "metadata": {},
   "source": [
    "### Model Arguments\n",
    "\n",
    "| Argument       | Meaning/Definition                                  | Notation in Mamba Paper                  |\n",
    "|----------------|-----------------------------------------------------|------------------------------------------|\n",
    "| `b`            | Batch size                                          | `B` in Algorithm 2                       |\n",
    "| `l`            | Sequence length                                     | `L` in Algorithm 2                       |\n",
    "| `d`, `d_model` | Hidden dimension                                    |                                          |\n",
    "| `n`, `d_state` | Latent state dimension                              | `N` in Algorithm 2                       |\n",
    "| `expand`       | Expansion factor                                    | `E` in Section 3.4                       |\n",
    "| `d_in`, `d_inner` | `d * expand` (expanded hidden dimension)         | `D` in Algorithm 2                       |\n",
    "| `A`, `B`, `C`, `D` | State space parameters                          | See state space formulas. `B`, `C` are input-dependent (selective); `A`, `D` are not |\n",
    "| `Δ`, `delta`   | Input-dependent step size                           |                                          |\n",
    "| `dt_rank`      | Rank of `Δ`                                         | See Section 3.6: \"Parameterization of ∆\" |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aa2c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    d_model: int\n",
    "    n_layer: int\n",
    "    vocab_size: int\n",
    "    d_state: int = 16\n",
    "    expand: int = 2\n",
    "    dt_rank: Union[int, str] = 'auto'\n",
    "    d_conv: int = 4 \n",
    "    pad_vocab_size_multiple: int = 8\n",
    "    conv_bias: bool = True\n",
    "    bias: bool = False\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.d_inner = int(self.expand * self.d_model)\n",
    "        \n",
    "        if self.dt_rank == 'auto':\n",
    "            self.dt_rank = math.ceil(self.d_model / 16)\n",
    "            \n",
    "        if self.vocab_size % self.pad_vocab_size_multiple != 0:\n",
    "            self.vocab_size += (self.pad_vocab_size_multiple\n",
    "                                - self.vocab_size % self.pad_vocab_size_multiple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311202da",
   "metadata": {},
   "source": [
    "### MAMBA Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e318f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"A single Mamba block, as described in Figure 3 in Section 3.4 in the Mamba paper [1].\"\"\"\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.in_proj = nn.Linear(args.d_model, args.d_inner * 2, bias=args.bias)\n",
    "\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=args.d_inner,\n",
    "            out_channels=args.d_inner,\n",
    "            bias=args.conv_bias,\n",
    "            kernel_size=args.d_conv,\n",
    "            groups=args.d_inner,\n",
    "            padding=args.d_conv - 1,\n",
    "        )\n",
    "\n",
    "        # x_proj takes in `x` and outputs the input-specific Δ, B, C\n",
    "        self.x_proj = nn.Linear(args.d_inner, args.dt_rank + args.d_state * 2, bias=False)\n",
    "        \n",
    "        # dt_proj projects Δ from dt_rank to d_in\n",
    "        self.dt_proj = nn.Linear(args.dt_rank, args.d_inner, bias=True)\n",
    "\n",
    "        A = repeat(torch.arange(1, args.d_state + 1), 'n -> d n', d=args.d_inner)\n",
    "        self.A_log = nn.Parameter(torch.log(A))\n",
    "        self.D = nn.Parameter(torch.ones(args.d_inner))\n",
    "        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=args.bias)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Mamba block forward. This looks the same as Figure 3 in Section 3.4 in the Mamba paper [1].\n",
    "    \n",
    "        Args:\n",
    "            x: shape (b, l, d)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "    \n",
    "        Returns:\n",
    "            output: shape (b, l, d)\n",
    "        \n",
    "        Official Implementation:\n",
    "            class Mamba, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L119\n",
    "            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n",
    "            \n",
    "        \"\"\"\n",
    "        (b, l, d) = x.shape\n",
    "        \n",
    "        x_and_res = self.in_proj(x)  # shape (b, l, 2 * d_in)\n",
    "        (x, res) = x_and_res.split(split_size=[self.args.d_inner, self.args.d_inner], dim=-1)\n",
    "\n",
    "        x = rearrange(x, 'b l d_in -> b d_in l')\n",
    "        x = self.conv1d(x)[:, :, :l]\n",
    "        x = rearrange(x, 'b d_in l -> b l d_in')\n",
    "        \n",
    "        x = F.silu(x)\n",
    "\n",
    "        y = ssm(x, self.A_log, self.D, self.x_proj, self.args.dt_rank, self.dt_proj)\n",
    "        \n",
    "        y = y * F.silu(res)\n",
    "        \n",
    "        output = self.out_proj(y)\n",
    "\n",
    "        return output    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f685dda9",
   "metadata": {},
   "source": [
    "### Residual Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d80e5422",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"Simple block wrapping Mamba block with normalization and residual connection.\"\"\"\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.mixer = MambaBlock(args)\n",
    "        self.norm = RMSNorm(args.d_model)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: shape (b, l, d)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "    \n",
    "        Returns:\n",
    "            output: shape (b, l, d)\n",
    "\n",
    "        Official Implementation:\n",
    "            Block.forward(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L297\n",
    "            \n",
    "            Note: the official repo chains residual blocks that look like\n",
    "                [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> ...\n",
    "            where the first Add is a no-op. This is purely for performance reasons as this\n",
    "            allows them to fuse the Add->Norm.\n",
    "\n",
    "            We instead implement our blocks as the more familiar, simpler, and numerically equivalent\n",
    "                [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> ....\n",
    "            \n",
    "        \"\"\"\n",
    "        output = self.mixer(self.norm(x)) + x\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebc5789",
   "metadata": {},
   "source": [
    "### MAMBA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "792f9b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mamba(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"Full Mamba model.\"\"\"\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        \n",
    "        self.embedding = nn.Embedding(args.vocab_size, args.d_model)\n",
    "        self.layers = nn.ModuleList([ResidualBlock(args) for _ in range(args.n_layer)])\n",
    "        self.norm_f = RMSNorm(args.d_model)\n",
    "\n",
    "        self.lm_head = nn.Linear(args.d_model, args.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.embedding.weight  # Tie output projection to embedding weights.\n",
    "                                                     # See \"Weight Tying\" paper\n",
    "\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids (long tensor): shape (b, l)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "    \n",
    "        Returns:\n",
    "            logits: shape (b, l, vocab_size)\n",
    "\n",
    "        Official Implementation:\n",
    "            class MambaLMHeadModel, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py#L173\n",
    "\n",
    "        \"\"\"\n",
    "        x = self.embedding(input_ids)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        x = self.norm_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c004aeb4",
   "metadata": {},
   "source": [
    "# Model Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54318334",
   "metadata": {},
   "source": [
    "### Loading the Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7677682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelArgs(d_model=1024, n_layer=48, vocab_size=50280, d_state=16, expand=2, dt_rank=64, d_conv=4, pad_vocab_size_multiple=8, conv_bias=True, bias=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Mamba(\n",
       "  (embedding): Embedding(50280, 1024)\n",
       "  (layers): ModuleList(\n",
       "    (0-47): 48 x ResidualBlock(\n",
       "      (mixer): MambaBlock(\n",
       "        (in_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "        (conv1d): Conv1d(2048, 2048, kernel_size=(4,), stride=(1,), padding=(3,), groups=2048)\n",
       "        (x_proj): Linear(in_features=2048, out_features=96, bias=False)\n",
       "        (dt_proj): Linear(in_features=64, out_features=2048, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "      )\n",
       "      (norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm_f): RMSNorm()\n",
       "  (lm_head): Linear(in_features=1024, out_features=50280, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a pretrained model\n",
    "args, state_dict = from_pretrained('state-spaces/mamba-370m')\n",
    "model = Mamba(ModelArgs(**args))\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Show model arguments and architecture\n",
    "display(ModelArgs(**args))\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1b57d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f0184b",
   "metadata": {},
   "source": [
    "### Generating Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5bae6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mamba is the second album to be released by the band. This album features the addition of former members of Black Keys – Jason Isbell and Scott Shamblin, and new drummer, James McBain. They recorded most of the album before being joined by Is"
     ]
    }
   ],
   "source": [
    "# Tokenize the input\n",
    "input_tokens = tokenizer(\"Mamba is the\", return_tensors='pt').input_ids\n",
    "\n",
    "# Generate tokens\n",
    "for token in generate(model, input_tokens, 50):\n",
    "    print(tokenizer.decode(token), end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
